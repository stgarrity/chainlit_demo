{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "wcd_url = os.environ[\"WCD_URL\"]\n",
    "wcd_api_key = os.environ[\"WCD_API_KEY\"]\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=wcd_url,                                    \n",
    "    auth_credentials=Auth.api_key(wcd_api_key),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.007731514051556587, 0.03945738077163696, -0.017062650993466377, -0.019995294511318207, 0.024583689868450165]\n",
      "\n",
      "\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "embeddings = embed_model.get_text_embedding(\n",
    "    \"OpenAI new Embeddings models is great.\"\n",
    ")\n",
    "\n",
    "print(embeddings[:5])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the docs in the data folder\n",
    "\n",
    "docs = SimpleDirectoryReader('./data').load_data()\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "\n",
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"Tesla\")\n",
    "\n",
    "# setting up the storage for the embeddings\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "# set up the index\n",
    "index = VectorStoreIndex(nodes, storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Tesla's velocity may be lower than usual due to a reduction in the vehicle's top speed and slower response to acceleration requests. Additionally, the maximum range of the vehicle may be reduced, and it may take longer to charge than before.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"reasons that my tesla's velocity is lower than usual\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"Tesla\")\n",
    "vector_store.delete_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
